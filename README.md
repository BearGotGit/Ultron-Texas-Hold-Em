# ğŸƒ Texas-Hold-Em Ultron Take Over

Texas Hold'em is a popular poker game played in all casinos.  
For our **4444 class project**, our group is building a **competitive Machine Learning poker bot** capable of playing Texas Hold'em at a high level â€” and hopefully better than you or I!

## ğŸ‘¥ Team Members

Anthony â€¢ Berend â€¢ Daniel â€¢ Dina â€¢ Eby â€¢ Aaron

![unnamed](https://github.com/user-attachments/assets/fab873ac-b36c-495e-88cb-53aca8cb5ca3)

---

## ğŸ¯ Project Goal

Build an AI agent that can:

- Understand game state  
- Evaluate hand strength & equity  
- Predict opponent ranges  
- Make decisions (fold/call/raise) in real time  
- Compete against other teams' bots in a class tournament  

Our approach uses **Monte Carlo simulation**, **supervised learning**, and **neural networks** to train a competitive Texas Hold'em model.

---

## Project Structure

```bash
Ultron-Texas-Hold-Em/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”œâ”€â”€ processed/
â”‚ â””â”€â”€ datasets.md
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ saved/
â”‚ â””â”€â”€ architecture/
â”‚
â”œâ”€â”€ simulation/
â”‚ â”œâ”€â”€ generate_dataset.py
â”‚ â”œâ”€â”€ card_utils.py
â”‚ â””â”€â”€ poker_simulator.py
â”‚
â”œâ”€â”€ training/
â”‚ â”œâ”€â”€ train_model.py
â”‚ â”œâ”€â”€ evaluate_model.py
â”‚ â””â”€â”€ losses.py
â”‚
â”œâ”€â”€ gameplay/
â”‚ â”œâ”€â”€ ai_agent.py
â”‚ â”œâ”€â”€ opponent_models.py
â”‚ â””â”€â”€ game_state.py
â”‚
â”œâ”€â”€ tools/
â”‚ â”œâ”€â”€ visualize.py
â”‚ â”œâ”€â”€ utils.py
â”‚ â””â”€â”€ config.py
â”‚
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_simulation.py
â”‚ â”œâ”€â”€ test_model.py
â”‚ â””â”€â”€ test_gameplay.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/BearGotGit/Ultron-Texas-Hold-Em
cd Ultron-Texas-Hold-Em

python -m venv .venv
# activate windows or mac way, then...
pip install -r requirements.txt
```

---

## Running the Project

### ğŸ® Play Against the Bot
```bash
python main.py
```

### ğŸ§  Train the RL Agent (PPO)

Train a poker bot using Proximal Policy Optimization:

```bash
# Quick test run (10k timesteps, ~30 seconds)
PYTHONPATH=. python training/train_rl_model.py --total-timesteps 10000

# Full training run (1M timesteps, ~1-2 hours on CPU)
PYTHONPATH=. python training/train_rl_model.py --total-timesteps 1000000

# Custom training options
PYTHONPATH=. python training/train_rl_model.py \
    --total-timesteps 500000 \
    --num-players 2 \
    --lr 0.0003 \
    --hidden-dim 256 \
    --run-name my_training_run

# Resume from checkpoint
PYTHONPATH=. python training/train_rl_model.py --resume checkpoints/checkpoint_100.pt
```

**Training Options:**
| Flag | Default | Description |
|------|---------|-------------|
| `--total-timesteps` | 100,000 | Total training decisions |
| `--num-players` | 2 | Players per table (hero + opponents) |
| `--lr` | 0.0003 | Learning rate |
| `--hidden-dim` | 256 | Neural network hidden layer size |
| `--run-name` | auto | Name for TensorBoard logs |
| `--resume` | None | Checkpoint path to resume from |

**Monitor Training with TensorBoard:**
```bash
tensorboard --logdir runs
```
Then open http://localhost:6006 in your browser.

**Checkpoints:** Saved to `checkpoints/` directory.

---

### ğŸ“Š Other Commands

1ï¸âƒ£ Generate Training Data (Monte Carlo simulation):
```bash
python simulation/generate_dataset.py
```

2ï¸âƒ£ Train Supervised Model (deprecated, use RL instead):
```bash
python training/train_model.py
```

3ï¸âƒ£ Evaluate Model Performance:
```bash
python training/evaluate_model.py
```
